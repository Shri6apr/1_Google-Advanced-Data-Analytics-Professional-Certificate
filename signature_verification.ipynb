{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8c71K29SXpQU/VKr0pr1N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shri6apr/1_Google-Advanced-Data-Analytics-Professional-Certificate/blob/main/signature_verification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsAneb-OLvi4"
      },
      "outputs": [],
      "source": [
        "# !pip install opencv-python-headless numpy tensorflow pillow sklearn matplotlib\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications import ResNet50V2\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "metadata": {
        "id": "MSXC7tTM3YpD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f496e49-9dda-4739-8a34-9b5db793b2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHJqlz8dPrpW",
        "outputId": "dcae1fd2-47a4-43b6-9e9c-88d5b75fe031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    def __init__(self):\n",
        "        self.DRIVE_PATH = '/content/drive/My Drive/data'\n",
        "        self.AUTHOR = '021'\n",
        "        self.TRAINING_FOLDER = os.path.join(self.DRIVE_PATH, 'training', self.AUTHOR)\n",
        "        self.TEST_FOLDER = os.path.join(self.DRIVE_PATH, 'test', self.AUTHOR)\n",
        "        self.INPUT_SHAPE = (128, 128, 1)\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.EPOCHS = 50\n",
        "        self.MAX_PAIRS_PER_CLASS = 1000\n",
        "\n",
        "config = Config()\n",
        "\n",
        "# Verify paths\n",
        "print(f\"Training folder: {config.TRAINING_FOLDER}\")\n",
        "print(f\"Test folder: {config.TEST_FOLDER}\")\n",
        "print(f\"Training folder exists: {os.path.exists(config.TRAINING_FOLDER)}\")\n",
        "print(f\"Test folder exists: {os.path.exists(config.TEST_FOLDER)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW-6TiSTUu4s",
        "outputId": "8eb56708-0394-4bcb-f9bb-02679bea2299"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training folder: /content/drive/My Drive/data/training/021\n",
            "Test folder: /content/drive/My Drive/data/test/021\n",
            "Training folder exists: True\n",
            "Test folder exists: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_images_from_folder(folder):\n",
        "    \"\"\"Load images and labels from a folder\"\"\"\n",
        "    images = []\n",
        "    labels = []\n",
        "    filenames = []\n",
        "\n",
        "    if not os.path.exists(folder):\n",
        "        print(f\"Directory {folder} does not exist.\")\n",
        "        return images, labels, filenames\n",
        "\n",
        "    for filename in os.listdir(folder):\n",
        "        try:\n",
        "            file_path = os.path.join(folder, filename)\n",
        "            img = cv2.imread(file_path)\n",
        "            if img is not None:\n",
        "                images.append(img)\n",
        "                # Add label based on filename\n",
        "                label = 1 if 'genuine' in filename.lower() else 0\n",
        "                labels.append(label)\n",
        "                filenames.append(filename)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading file {file_path}: {e}\")\n",
        "\n",
        "    print(f\"Total images loaded from {folder}: {len(images)}\")\n",
        "    return images, labels, filenames\n",
        "\n",
        "class SignatureVerificationSystem:\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.input_shape = config.INPUT_SHAPE\n",
        "        self.model = self.build_efficient_siamese_model()\n",
        "\n",
        "    def preprocess_image(self, img):\n",
        "        \"\"\"Preprocess a single image\"\"\"\n",
        "        try:\n",
        "            # Convert to grayscale if needed\n",
        "            if len(img.shape) == 3:\n",
        "                img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "            # Resize\n",
        "            img = cv2.resize(img, (self.input_shape[0], self.input_shape[1]))\n",
        "\n",
        "            # Apply CLAHE\n",
        "            clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
        "            img = clahe.apply(img.astype(np.uint8))\n",
        "\n",
        "            # Normalize and add channel dimension\n",
        "            img = (img / 255.0).astype(np.float32)\n",
        "            img = np.expand_dims(img, axis=-1)\n",
        "\n",
        "            return img\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error preprocessing image: {str(e)}\")\n",
        "            return None\n",
        "\n",
        "    def preprocess_dataset(self, images):\n",
        "        \"\"\"Preprocess a batch of images\"\"\"\n",
        "        processed_images = []\n",
        "        for img in images:\n",
        "            processed_img = self.preprocess_image(img)\n",
        "            if processed_img is not None:\n",
        "                processed_images.append(processed_img)\n",
        "\n",
        "        return np.array(processed_images)\n",
        "\n",
        "    def build_efficient_siamese_model(self):\n",
        "        \"\"\"Build the Siamese network with ResNet50V2 backbone\"\"\"\n",
        "        # Initialize ResNet50V2\n",
        "        base_model = ResNet50V2(\n",
        "            include_top=False,\n",
        "            weights='imagenet',\n",
        "            input_shape=(self.input_shape[0], self.input_shape[1], 3),\n",
        "            pooling='avg'\n",
        "        )\n",
        "\n",
        "        # Freeze early layers\n",
        "        for layer in base_model.layers[:-30]:\n",
        "            layer.trainable = False\n",
        "\n",
        "        def create_embedding_network():\n",
        "            input_layer = layers.Input(shape=self.input_shape)\n",
        "\n",
        "            # Convert grayscale to 3 channels\n",
        "            x = layers.Conv2D(3, (1, 1))(input_layer)\n",
        "\n",
        "            # Pass through ResNet\n",
        "            x = base_model(x)\n",
        "\n",
        "            # Add modern layers\n",
        "            x = layers.Dense(512, activation='selu')(x)\n",
        "            x = layers.BatchNormalization()(x)\n",
        "            x = layers.Dropout(0.3)(x)\n",
        "            x = layers.Dense(256, activation='selu')(x)\n",
        "\n",
        "            return Model(input_layer, x)\n",
        "\n",
        "        embedding_network = create_embedding_network()\n",
        "\n",
        "        # Create Siamese network\n",
        "        input_a = layers.Input(shape=self.input_shape)\n",
        "        input_b = layers.Input(shape=self.input_shape)\n",
        "\n",
        "        embedding_a = embedding_network(input_a)\n",
        "        embedding_b = embedding_network(input_b)\n",
        "\n",
        "        # Cosine similarity\n",
        "        cosine_similarity = layers.Dot(axes=1, normalize=True)([embedding_a, embedding_b])\n",
        "\n",
        "        # Attention mechanism\n",
        "        attention = layers.Concatenate()([embedding_a, embedding_b])\n",
        "        attention = layers.Dense(64, activation='relu')(attention)\n",
        "        attention = layers.Dense(1, activation='sigmoid')(attention)\n",
        "\n",
        "        # Combine similarity and attention\n",
        "        output = layers.Average()([cosine_similarity, attention])\n",
        "\n",
        "        model = Model(inputs=[input_a, input_b], outputs=output)\n",
        "        return model\n",
        "\n",
        "    def create_pairs(self, images, labels):\n",
        "        \"\"\"Create balanced pairs for training\"\"\"\n",
        "        genuine_indices = np.where(np.array(labels) == 1)[0]\n",
        "        forged_indices = np.where(np.array(labels) == 0)[0]\n",
        "\n",
        "        pairs = []\n",
        "        pair_labels = []\n",
        "\n",
        "        # Create positive pairs (genuine-genuine)\n",
        "        for i in range(len(genuine_indices)):\n",
        "            for j in range(i + 1, len(genuine_indices)):\n",
        "                pairs.append([images[genuine_indices[i]], images[genuine_indices[j]]])\n",
        "                pair_labels.append(1)\n",
        "                if len(pairs) >= self.config.MAX_PAIRS_PER_CLASS:\n",
        "                    break\n",
        "            if len(pairs) >= self.config.MAX_PAIRS_PER_CLASS:\n",
        "                break\n",
        "\n",
        "        # Create negative pairs (genuine-forged)\n",
        "        num_negative = min(len(pairs), len(forged_indices) * len(genuine_indices))\n",
        "        for _ in range(num_negative):\n",
        "            genuine_idx = np.random.choice(genuine_indices)\n",
        "            forged_idx = np.random.choice(forged_indices)\n",
        "            pairs.append([images[genuine_idx], images[forged_idx]])\n",
        "            pair_labels.append(0)\n",
        "\n",
        "        return np.array(pairs), np.array(pair_labels)\n",
        "\n",
        "    def train(self, train_pairs, train_labels, validation_data=None):\n",
        "        \"\"\"Train the model with advanced techniques\"\"\"\n",
        "        callbacks = [\n",
        "            ReduceLROnPlateau(\n",
        "                monitor='val_loss',\n",
        "                factor=0.5,\n",
        "                patience=3,\n",
        "                min_lr=1e-6,\n",
        "                verbose=1\n",
        "            ),\n",
        "            EarlyStopping(\n",
        "                monitor='val_loss',\n",
        "                patience=10,\n",
        "                restore_best_weights=True\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        self.model.compile(\n",
        "            optimizer=Adam(learning_rate=1e-3),\n",
        "            loss='binary_crossentropy',\n",
        "            metrics=['accuracy', tf.keras.metrics.AUC()]\n",
        "        )\n",
        "\n",
        "        history = self.model.fit(\n",
        "            [train_pairs[:, 0], train_pairs[:, 1]],\n",
        "            train_labels,\n",
        "            validation_data=validation_data,\n",
        "            epochs=self.config.EPOCHS,\n",
        "            batch_size=self.config.BATCH_SIZE,\n",
        "            callbacks=callbacks\n",
        "        )\n",
        "\n",
        "        return history\n"
      ],
      "metadata": {
        "id": "mpaHmA38Pz-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Initialize system\n",
        "    system = SignatureVerificationSystem(config)\n",
        "\n",
        "    # Load training and test images\n",
        "    print(\"Loading training images...\")\n",
        "    training_images, training_labels, training_filenames = load_images_from_folder(config.TRAINING_FOLDER)\n",
        "    print(\"Loading test images...\")\n",
        "    test_images, test_labels, test_filenames = load_images_from_folder(config.TEST_FOLDER)\n",
        "\n",
        "    # Preprocess images\n",
        "    print(\"Preprocessing training images...\")\n",
        "    training_data = system.preprocess_dataset(training_images)\n",
        "    print(\"Preprocessing test images...\")\n",
        "    test_data = system.preprocess_dataset(test_images)\n",
        "\n",
        "    # Create pairs\n",
        "    print(\"Creating pairs...\")\n",
        "    train_pairs, train_pair_labels = system.create_pairs(training_data, training_labels)\n",
        "    test_pairs, test_pair_labels = system.create_pairs(test_data, test_labels)\n",
        "\n",
        "    print(f\"Training pairs shape: {train_pairs.shape}\")\n",
        "    print(f\"Test pairs shape: {test_pairs.shape}\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"Training model...\")\n",
        "    history = system.train(\n",
        "        train_pairs,\n",
        "        train_pair_labels,\n",
        "        validation_data=([test_pairs[:, 0], test_pairs[:, 1]], test_pair_labels)\n",
        "    )\n",
        "\n",
        "    # Save model\n",
        "    model_save_path = os.path.join(config.DRIVE_PATH, f'signature_model_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}.h5')\n",
        "    system.model.save(model_save_path)\n",
        "    print(f\"Model saved to: {model_save_path}\")\n",
        "\n",
        "    # Print final metrics\n",
        "    test_loss, test_accuracy, test_auc = system.model.evaluate(\n",
        "        [test_pairs[:, 0], test_pairs[:, 1]],\n",
        "        test_pair_labels\n",
        "    )\n",
        "    print(f\"\\nFinal Test Metrics:\")\n",
        "    print(f\"Loss: {test_loss:.4f}\")\n",
        "    print(f\"Accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"AUC: {test_auc:.4f}\")\n"
      ],
      "metadata": {
        "id": "YIOLLAE8QD77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "LLUaO4GvL-0U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c5eab67-481e-4b77-a361-74b4e5fd09c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m94668760/94668760\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "Loading training images...\n",
            "Total images loaded from /content/drive/My Drive/data/training/021: 24\n",
            "Loading test images...\n",
            "Total images loaded from /content/drive/My Drive/data/test/021: 24\n",
            "Preprocessing training images...\n",
            "Preprocessing test images...\n",
            "Creating pairs...\n",
            "Training pairs shape: (132, 2, 128, 128, 1)\n",
            "Test pairs shape: (132, 2, 128, 128, 1)\n",
            "Training model...\n",
            "Epoch 1/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 13s/step - accuracy: 0.6303 - auc: 0.7229 - loss: 0.6060 - val_accuracy: 0.7045 - val_auc: 0.9521 - val_loss: 0.6414 - learning_rate: 0.0010\n",
            "Epoch 2/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 13s/step - accuracy: 0.7374 - auc: 0.9837 - loss: 0.4068 - val_accuracy: 0.8561 - val_auc: 0.9812 - val_loss: 0.3849 - learning_rate: 0.0010\n",
            "Epoch 3/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 13s/step - accuracy: 0.8309 - auc: 0.9932 - loss: 0.3196 - val_accuracy: 0.8333 - val_auc: 0.9860 - val_loss: 0.4950 - learning_rate: 0.0010\n",
            "Epoch 4/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 13s/step - accuracy: 0.8077 - auc: 0.9820 - loss: 0.3199 - val_accuracy: 0.9015 - val_auc: 0.9869 - val_loss: 0.3542 - learning_rate: 0.0010\n",
            "Epoch 5/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m77s\u001b[0m 11s/step - accuracy: 0.8823 - auc: 0.9970 - loss: 0.2373 - val_accuracy: 0.9015 - val_auc: 0.9860 - val_loss: 0.3049 - learning_rate: 0.0010\n",
            "Epoch 6/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 13s/step - accuracy: 0.8718 - auc: 0.9996 - loss: 0.2334 - val_accuracy: 0.9242 - val_auc: 0.9667 - val_loss: 0.2824 - learning_rate: 0.0010\n",
            "Epoch 7/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 13s/step - accuracy: 0.9158 - auc: 1.0000 - loss: 0.2051 - val_accuracy: 0.9394 - val_auc: 0.9804 - val_loss: 0.2158 - learning_rate: 0.0010\n",
            "Epoch 8/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 13s/step - accuracy: 0.9111 - auc: 0.9999 - loss: 0.1894 - val_accuracy: 0.9394 - val_auc: 0.9921 - val_loss: 0.1781 - learning_rate: 0.0010\n",
            "Epoch 9/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 13s/step - accuracy: 0.9141 - auc: 0.9999 - loss: 0.1695 - val_accuracy: 0.9394 - val_auc: 0.9924 - val_loss: 0.1978 - learning_rate: 0.0010\n",
            "Epoch 10/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 11s/step - accuracy: 0.9170 - auc: 0.9912 - loss: 0.2803 - val_accuracy: 0.9394 - val_auc: 0.9882 - val_loss: 0.2171 - learning_rate: 0.0010\n",
            "Epoch 11/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.9618 - auc: 1.0000 - loss: 0.1230 \n",
            "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 13s/step - accuracy: 0.9593 - auc: 1.0000 - loss: 0.1239 - val_accuracy: 0.9394 - val_auc: 0.9823 - val_loss: 0.2399 - learning_rate: 0.0010\n",
            "Epoch 12/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m76s\u001b[0m 11s/step - accuracy: 0.9524 - auc: 1.0000 - loss: 0.1044 - val_accuracy: 0.9394 - val_auc: 0.9823 - val_loss: 0.1699 - learning_rate: 5.0000e-04\n",
            "Epoch 13/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 13s/step - accuracy: 0.9476 - auc: 1.0000 - loss: 0.0992 - val_accuracy: 0.9848 - val_auc: 0.9888 - val_loss: 0.1099 - learning_rate: 5.0000e-04\n",
            "Epoch 14/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 13s/step - accuracy: 0.9463 - auc: 1.0000 - loss: 0.0923 - val_accuracy: 0.9848 - val_auc: 0.9884 - val_loss: 0.0890 - learning_rate: 5.0000e-04\n",
            "Epoch 15/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m73s\u001b[0m 11s/step - accuracy: 0.9429 - auc: 1.0000 - loss: 0.0924 - val_accuracy: 0.9848 - val_auc: 0.9884 - val_loss: 0.0851 - learning_rate: 5.0000e-04\n",
            "Epoch 16/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 11s/step - accuracy: 0.9422 - auc: 0.9972 - loss: 0.1304 - val_accuracy: 0.9091 - val_auc: 0.9335 - val_loss: 0.9232 - learning_rate: 5.0000e-04\n",
            "Epoch 17/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 13s/step - accuracy: 0.9281 - auc: 0.9994 - loss: 0.1217 - val_accuracy: 0.8106 - val_auc: 0.8905 - val_loss: 1.0884 - learning_rate: 5.0000e-04\n",
            "Epoch 18/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8607 - auc: 0.9693 - loss: 0.6241 \n",
            "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 13s/step - accuracy: 0.8625 - auc: 0.9716 - loss: 0.5874 - val_accuracy: 0.7121 - val_auc: 0.8237 - val_loss: 1.8798 - learning_rate: 5.0000e-04\n",
            "Epoch 19/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 13s/step - accuracy: 0.8560 - auc: 0.9919 - loss: 0.2071 - val_accuracy: 0.7348 - val_auc: 0.8487 - val_loss: 1.2208 - learning_rate: 2.5000e-04\n",
            "Epoch 20/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m72s\u001b[0m 11s/step - accuracy: 0.8520 - auc: 0.9944 - loss: 0.1870 - val_accuracy: 0.8106 - val_auc: 0.8873 - val_loss: 1.0863 - learning_rate: 2.5000e-04\n",
            "Epoch 21/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8572 - auc: 0.9999 - loss: 0.1730  \n",
            "Epoch 21: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 11s/step - accuracy: 0.8545 - auc: 0.9998 - loss: 0.1778 - val_accuracy: 0.9167 - val_auc: 0.9854 - val_loss: 0.1803 - learning_rate: 2.5000e-04\n",
            "Epoch 22/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 12s/step - accuracy: 0.8707 - auc: 1.0000 - loss: 0.1571 - val_accuracy: 0.9848 - val_auc: 0.9949 - val_loss: 0.1561 - learning_rate: 1.2500e-04\n",
            "Epoch 23/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 11s/step - accuracy: 0.9160 - auc: 1.0000 - loss: 0.1292 - val_accuracy: 0.9848 - val_auc: 0.9948 - val_loss: 0.1371 - learning_rate: 1.2500e-04\n",
            "Epoch 24/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.8965 - auc: 1.0000 - loss: 0.1318  \n",
            "Epoch 24: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 13s/step - accuracy: 0.8948 - auc: 1.0000 - loss: 0.1331 - val_accuracy: 0.9848 - val_auc: 0.9943 - val_loss: 0.1257 - learning_rate: 1.2500e-04\n",
            "Epoch 25/50\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 13s/step - accuracy: 0.8969 - auc: 1.0000 - loss: 0.1455 - val_accuracy: 0.9848 - val_auc: 0.9940 - val_loss: 0.1137 - learning_rate: 6.2500e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved to: /content/drive/My Drive/data/signature_model_20250125_150650.h5\n",
            "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4s/step - accuracy: 0.9906 - auc: 0.6598 - loss: 0.0555\n",
            "\n",
            "Final Test Metrics:\n",
            "Loss: 0.0851\n",
            "Accuracy: 0.9848\n",
            "AUC: 0.9884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "id": "zSwz2caF3aab",
        "outputId": "dbab34d1-87a4-410b-f863-36ed38ae5251"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e7aba7b5f1da>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"signature_verification_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oAItP3lkzwVC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}